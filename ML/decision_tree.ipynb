{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "spark2 = SparkSession.builder.appName('ml').getOrCreate()\n",
    "#Create a Spark Session\n",
    "SpSession = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"ml\") \\\n",
    "    .config(\"spark.executor.memory\", \"0.1g\") \\\n",
    "    .config(\"spark.cores.max\",\"2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/home/sushant/Projects/Spark_Project/temp\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "#Get the Spark Context from Spark Session    \n",
    "SpContext = SpSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PETAL_LENGTH: double, PETAL_WIDTH: double, SEPAL_LENGTH: double, SEPAL_WIDTH: double, SPECIES: string, IND_SPECIES: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Load Data\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#Load the CSV file into a RDD\n",
    "irisData = SpContext.textFile(\"iris.csv\")\n",
    "irisData.cache()\n",
    "irisData.count()\n",
    "\n",
    "#Remove the first line (contains headers)\n",
    "dataLines = irisData.filter(lambda x: \"Sepal\" not in x)\n",
    "dataLines.count()\n",
    "\n",
    "\"\"\"--------------------------------------------------------------------------\n",
    "Cleanup Data\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "from pyspark.sql import Row\n",
    "#Create a Data Frame from the data\n",
    "parts = dataLines.map(lambda l: l.split(\",\"))\n",
    "irisMap = parts.map(lambda p: Row(SEPAL_LENGTH=float(p[0]),\\\n",
    "                                SEPAL_WIDTH=float(p[1]), \\\n",
    "                                PETAL_LENGTH=float(p[2]), \\\n",
    "                                PETAL_WIDTH=float(p[3]), \\\n",
    "                                SPECIES=p[4] ))\n",
    "                                \n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "irisDf = SpSession.createDataFrame(irisMap)\n",
    "irisDf.cache()\n",
    "#Add a numeric indexer for the label/target column\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"SPECIES\", outputCol=\"IND_SPECIES\")\n",
    "si_model = stringIndexer.fit(irisDf)\n",
    "irisNormDf = si_model.transform(irisDf)\n",
    "\n",
    "irisNormDf.select(\"SPECIES\",\"IND_SPECIES\").distinct().collect()\n",
    "irisNormDf.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-------------------+---------+------------------+\n",
      "|summary|      PETAL_LENGTH|       PETAL_WIDTH|      SEPAL_LENGTH|        SEPAL_WIDTH|  SPECIES|       IND_SPECIES|\n",
      "+-------+------------------+------------------+------------------+-------------------+---------+------------------+\n",
      "|  count|               150|               150|               150|                150|      150|               150|\n",
      "|   mean|3.7580000000000027| 1.199333333333334| 5.843333333333335|  3.057333333333334|     null|               1.0|\n",
      "| stddev|1.7652982332594662|0.7622376689603467|0.8280661279778637|0.43586628493669793|     null|0.8192319205190403|\n",
      "|    min|               1.0|               0.1|               4.3|                2.0|   setosa|               0.0|\n",
      "|    max|               6.9|               2.5|               7.9|                4.4|virginica|               2.0|\n",
      "+-------+------------------+------------------+------------------+-------------------+---------+------------------+\n",
      "\n",
      "('Correlation to Species for ', 'PETAL_LENGTH')\n",
      "('Correlation to Species for ', 'PETAL_WIDTH')\n",
      "('Correlation to Species for ', 'SEPAL_LENGTH')\n",
      "('Correlation to Species for ', 'SEPAL_WIDTH')\n",
      "('Correlation to Species for ', 'SPECIES')\n",
      "('Correlation to Species for ', 'IND_SPECIES')\n",
      "+-------+-----+-----------------+\n",
      "|species|label|         features|\n",
      "+-------+-----+-----------------+\n",
      "| setosa|  2.0|[5.1,3.5,1.4,0.2]|\n",
      "| setosa|  2.0|[4.9,3.0,1.4,0.2]|\n",
      "| setosa|  2.0|[4.7,3.2,1.3,0.2]|\n",
      "| setosa|  2.0|[4.6,3.1,1.5,0.2]|\n",
      "| setosa|  2.0|[5.0,3.6,1.4,0.2]|\n",
      "| setosa|  2.0|[5.4,3.9,1.7,0.4]|\n",
      "| setosa|  2.0|[4.6,3.4,1.4,0.3]|\n",
      "| setosa|  2.0|[5.0,3.4,1.5,0.2]|\n",
      "| setosa|  2.0|[4.4,2.9,1.4,0.2]|\n",
      "| setosa|  2.0|[4.9,3.1,1.5,0.1]|\n",
      "+-------+-----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[species: string, label: double, features: vector]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Perform Data Analytics\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#See standard parameters\n",
    "irisNormDf.describe().show()\n",
    "\n",
    "#Find correlation between predictors and target\n",
    "for i in irisNormDf.columns:\n",
    "    if not( isinstance(irisNormDf.select(i).take(1)[0][0], str)) :\n",
    "        print( \"Correlation to Species for \", i)#, \\\n",
    "                    #irisNormDf.stat.corr('IND_SPECIES',i))\n",
    "\n",
    "\"\"\"--------------------------------------------------------------------------\n",
    "Prepare data for ML\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#Transform to a Data Frame for input to Machine Learing\n",
    "#Drop columns that are not required (low correlation)\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "def transformToLabeledPoint(row) :\n",
    "    lp = ( row[\"SPECIES\"], row[\"IND_SPECIES\"], \\\n",
    "                Vectors.dense([row[\"SEPAL_LENGTH\"],\\\n",
    "                        row[\"SEPAL_WIDTH\"], \\\n",
    "                        row[\"PETAL_LENGTH\"], \\\n",
    "                        row[\"PETAL_WIDTH\"]]))\n",
    "    return lp\n",
    "    \n",
    "irisLp = irisNormDf.rdd.map(transformToLabeledPoint)\n",
    "irisLpDf = SpSession.createDataFrame(irisLp,[\"species\",\"label\", \"features\"])\n",
    "irisLpDf.select(\"species\",\"label\",\"features\").show(10)\n",
    "irisLpDf.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|    2|\n",
      "|  2.0|       2.0|    6|\n",
      "|  0.0|       0.0|    3|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Perform Machine Learning\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "#Split into training and testing data\n",
    "(trainingData, testData) = irisLpDf.randomSplit([0.9, 0.1])\n",
    "trainingData.count()\n",
    "testData.count()\n",
    "testData.collect()\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Create the model\n",
    "dtClassifer = DecisionTreeClassifier(maxDepth=2, labelCol=\"label\",\\\n",
    "                featuresCol=\"features\")\n",
    "dtModel = dtClassifer.fit(trainingData)\n",
    "\n",
    "dtModel.numNodes\n",
    "dtModel.depth\n",
    "\n",
    "#Predict on the test data\n",
    "predictions = dtModel.transform(testData)\n",
    "predictions.select(\"prediction\",\"species\",\"label\").collect()\n",
    "\n",
    "#Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "evaluator.evaluate(predictions)\n",
    "\n",
    "\n",
    "#Draw a confusion matrix\n",
    "predictions.groupBy(\"label\",\"prediction\").count().show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
