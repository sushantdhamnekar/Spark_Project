{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "spark2 = SparkSession.builder.appName('ml').getOrCreate()\n",
    "#Create a Spark Session\n",
    "SpSession = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"ml\") \\\n",
    "    .config(\"spark.executor.memory\", \"0.1g\") \\\n",
    "    .config(\"spark.cores.max\",\"2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/home/sushant/Projects/Spark_Project/temp\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "#Get the Spark Context from Spark Session    \n",
    "SpContext = SpSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+----+------+\n",
      "|BODY|DOORS|  HP| MPG|   RPM|\n",
      "+----+-----+----+----+------+\n",
      "| 2.0|  1.0|69.0|31.0|4900.0|\n",
      "| 2.0|  1.0|48.0|47.0|5100.0|\n",
      "| 2.0|  1.0|68.0|30.0|5000.0|\n",
      "| 2.0|  1.0|62.0|35.0|4800.0|\n",
      "| 2.0|  1.0|68.0|37.0|5500.0|\n",
      "| 2.0|  1.0|60.0|38.0|5500.0|\n",
      "| 1.0|  1.0|69.0|31.0|5200.0|\n",
      "| 2.0|  1.0|68.0|37.0|5500.0|\n",
      "| 2.0|  1.0|68.0|37.0|5500.0|\n",
      "| 2.0|  1.0|68.0|31.0|5000.0|\n",
      "| 2.0|  1.0|68.0|31.0|5500.0|\n",
      "| 2.0|  2.0|68.0|31.0|5500.0|\n",
      "| 2.0|  2.0|68.0|31.0|5500.0|\n",
      "| 2.0|  1.0|70.0|38.0|5400.0|\n",
      "| 2.0|  1.0|62.0|31.0|4800.0|\n",
      "| 2.0|  1.0|68.0|31.0|5500.0|\n",
      "| 2.0|  1.0|58.0|49.0|4800.0|\n",
      "| 2.0|  2.0|62.0|31.0|4800.0|\n",
      "| 2.0|  1.0|76.0|30.0|6000.0|\n",
      "| 1.0|  2.0|70.0|38.0|5400.0|\n",
      "+----+-----+----+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the CSV file into a RDD\n",
    "autoData = SpContext.textFile(\"auto-data.csv\")\n",
    "autoData.cache()\n",
    "\n",
    "#Remove the first line (contains headers)\n",
    "firstLine = autoData.first()\n",
    "dataLines = autoData.filter(lambda x: x != firstLine)\n",
    "dataLines.count()\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import math\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#Convert to Local Vector.\n",
    "def transformToNumeric( inputStr) :\n",
    "    attList=inputStr.split(\",\")\n",
    "\n",
    "    doors = 1.0 if attList[3] ==\"two\" else 2.0\n",
    "    body = 1.0 if attList[4] == \"sedan\" else 2.0 \n",
    "       \n",
    "    #Filter out columns not wanted at this stage\n",
    "    values= Row(DOORS= doors, \\\n",
    "                     BODY=float(body),  \\\n",
    "                     HP=float(attList[7]),  \\\n",
    "                     RPM=float(attList[8]),  \\\n",
    "                     MPG=float(attList[9])  \\\n",
    "                     )\n",
    "    return values\n",
    "\n",
    "autoMap = dataLines.map(transformToNumeric)\n",
    "autoMap.persist()\n",
    "autoMap.collect()\n",
    "\n",
    "autoDf = SpSession.createDataFrame(autoMap)\n",
    "autoDf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.93367168148051...|\n",
      "|[0.93367168148051...|\n",
      "|[0.93367168148051...|\n",
      "|[0.93367168148051...|\n",
      "|[0.93367168148051...|\n",
      "|[0.93367168148051...|\n",
      "|[-1.0656035495158...|\n",
      "|[0.93367168148051...|\n",
      "|[0.93367168148051...|\n",
      "|[0.93367168148051...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Centering and scaling. To perform this every value should be subtracted\n",
    "#from that column's mean and divided by its Std. Deviation.\n",
    "\n",
    "summStats=autoDf.describe().toPandas()\n",
    "meanValues=summStats.iloc[1,1:5].values.tolist()\n",
    "stdValues=summStats.iloc[2,1:5].values.tolist()\n",
    "\n",
    "#place the means and std.dev values in a broadcast variable\n",
    "bcMeans=SpContext.broadcast(meanValues)\n",
    "bcStdDev=SpContext.broadcast(stdValues)\n",
    "\n",
    "def centerAndScale(inRow) :\n",
    "    global bcMeans\n",
    "    global bcStdDev\n",
    "    \n",
    "    meanArray=bcMeans.value\n",
    "    stdArray=bcStdDev.value\n",
    "\n",
    "    retArray=[]\n",
    "    for i in range(len(meanArray)):\n",
    "        retArray.append( (float(inRow[i]) - float(meanArray[i])) /\\\n",
    "            float(stdArray[i]) )\n",
    "    return Vectors.dense(retArray)\n",
    "    \n",
    "csAuto = autoDf.rdd.map(centerAndScale)\n",
    "csAuto.collect()\n",
    "\n",
    "#Create a Spark Data Frame\n",
    "autoRows=csAuto.map( lambda f:Row(features=f))\n",
    "autoDf = SpSession.createDataFrame(autoRows)\n",
    "\n",
    "autoDf.select(\"features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[-1.0656035495158...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         1|\n",
      "|[0.93367168148051...|         1|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         2|\n",
      "|[0.93367168148051...|         1|\n",
      "|[0.93367168148051...|         2|\n",
      "|[-1.0656035495158...|         1|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(k=3, seed=1)\n",
    "model = kmeans.fit(autoDf)\n",
    "predictions = model.transform(autoDf)\n",
    "predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
