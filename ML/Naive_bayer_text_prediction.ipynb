{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "spark2 = SparkSession.builder.appName('ml').getOrCreate()\n",
    "#Create a Spark Session\n",
    "SpSession = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"ml\") \\\n",
    "    .config(\"spark.executor.memory\", \"0.1g\") \\\n",
    "    .config(\"spark.cores.max\",\"2\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/home/sushant/Projects/Spark_Project/temp\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "#Get the Spark Context from Spark Session    \n",
    "SpContext = SpSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|             message|\n",
      "+-----+--------------------+\n",
      "|  0.0|Go until jurong p...|\n",
      "|  0.0|Ok lar... Joking ...|\n",
      "|  0.0|U dun say so earl...|\n",
      "|  0.0|Nah I don't think...|\n",
      "|  0.0|Even my brother i...|\n",
      "|  0.0|As per your reque...|\n",
      "|  0.0|I'm gonna be home...|\n",
      "|  0.0|I've been searchi...|\n",
      "|  0.0|I HAVE A DATE ON ...|\n",
      "|  0.0|Oh k...i'm watchi...|\n",
      "|  0.0|Eh u remember how...|\n",
      "|  0.0|Fine if thats th...|\n",
      "|  0.0|Is that seriously...|\n",
      "|  0.0|I‘m going to try ...|\n",
      "|  0.0|So ü pay first la...|\n",
      "|  0.0|Aft i finish my l...|\n",
      "|  0.0|Ffffffffff. Alrig...|\n",
      "|  0.0|Just forced mysel...|\n",
      "|  0.0|Lol your always s...|\n",
      "|  0.0|Did you catch the...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Load Data\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "#Load the CSV file into a RDD\n",
    "smsData = SpContext.textFile(\"SMSSpamCollection.csv\",2)\n",
    "smsData.cache()\n",
    "smsData.collect()\n",
    "\n",
    "\"\"\"--------------------------------------------------------------------------\n",
    "Prepare data for ML\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "def TransformToVector(inputStr):\n",
    "    attList=inputStr.split(\",\")\n",
    "    smsType= 0.0 if attList[0] == \"ham\" else 1.0\n",
    "    return [smsType, attList[1]]\n",
    "\n",
    "smsXformed=smsData.map(TransformToVector)\n",
    "\n",
    "smsDf= SpSession.createDataFrame(smsXformed,\n",
    "                          [\"label\",\"message\"])\n",
    "smsDf.cache()\n",
    "smsDf.select(\"label\",\"message\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   51|\n",
      "|  0.0|       1.0|    4|\n",
      "|  1.0|       0.0|    1|\n",
      "|  0.0|       0.0|   50|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Perform Machine Learning\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "#Split training and testing\n",
    "(trainingData, testData) = smsDf.randomSplit([0.9, 0.1])\n",
    "trainingData.count()\n",
    "testData.count()\n",
    "testData.collect()\n",
    "\n",
    "#Setup pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Split into words and then build TF-IDF\n",
    "tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), \\\n",
    "        outputCol=\"tempfeatures\")\n",
    "idf=IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "nbClassifier=NaiveBayes()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, \\\n",
    "                idf, nbClassifier])\n",
    "\n",
    "#Build a model with a pipeline\n",
    "nbModel=pipeline.fit(trainingData)\n",
    "#Predict on test data\n",
    "prediction=nbModel.transform(testData)\n",
    "\n",
    "#Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "evaluator.evaluate(prediction)\n",
    "\n",
    "#Draw confusion matrics\n",
    "prediction.groupBy(\"label\",\"prediction\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
